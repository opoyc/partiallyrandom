---
title: Power analysis in experimental design
author: Obryan Poyser
date: '2018-07-15'
slug: power-analysis-in-experimental-design
categories:
  - Statistics
tags:
  - Power Analysis
  - R
  - Experiments
  - Causal inference
type: post
description: ""
keywords:
  - key
  - words
topics:
  - topic 1
bibliography: allrefs.bib
lang:
  - english
---

A couple of weeks ago we had hour yearly Ph.D. workshop in which we present our project development. I've always moral sentiments around my duties regarding what someone like me who received a grant should do. One of these moral burdens is to assist to every single presentation and try to provide useful feedback.

Long story short, a fellow Ph.D. student presented a draft of a experimental design. Aligned with my principles, I suggested her to be more humble regarding what she wanted to test, since there were far too many hypotheses she wanted to test for (i.e. treatments) which certainly would limit her sample within and between groups. As a consequence, she replied: *How big do you think my sample should be?*, to be honest, I couldn't provide her with an exact answer, this post is a result of my deficient feedback.

Experiments have been increasing in popularity in economics, as an effect of the "empirical revolution" trending as a whole as well as an increasing criticism on identification assumptions when the objective is to find a causal effect in observational data. Indeed, experiments let us identify a cause for certain phenomena easier, nevertheless, the design as well as the cost of realizing an experiment is not negligible. If something ends bad, most probably we won't have the same resources and opportunity to rerun it, hence, we must be careful to design an experiment that maximizes our testing expectations.

Finding the optimal sample size is definitely one of the main concerns a researcher should have when she designs an experiment. For this matter, there exist the so-called **Power Analysis**, a process of estimating the minimum (optimal) sample size needed to detect an effect given a treatment at the desired significance level.


@List2011[] argue that there are three main aspects we should consider in order to calculate the optimal sample size:

1. Significance level
1. Power
1. Minimum detectable effect size

So, let's start by talking about significance level, by bringing back what we learned in statistics 101 about hypothesis testing:

+ Type I: Probability of rejecting the null hypothesis given that it is true (false positive). Typically, it is referred as significance level $\alpha$ of the hypothesis test.
$$P(R|H_0=true)$$

+ Type II: Probability of not rejecting (accepting for notational purposes) the null hypothesis given that it is false (false negative). Commonly, this decision error is described as $\beta$.
$$P(A|H_0=false)$$

The power is defined as the likelihood of correctly rejecting $H_0$ when it was actually false, or $1-\beta=1-P(A|H_0=false)$. The rule of thumb dictates a power equivalent of $0.80$. Having all this information, it will be useful demostrate the use of power with an example for a couple of syntethic random variables. Let's assume we run an experiment to test for the effect of advertising on monthly chocolate consumption, described as two normal distributions with $\mu_1\sim N(40,5)$ and $\mu_2 \sim N(50,5)$ for a control and treatment groups respectively.

```{r, warning=FALSE, message=FALSE}
require(tidyverse)
require(ggridges)

set.seed(29)

# Two normal random variables with mean 40 and 50 respectively.
control <- rnorm(n = 120, 40, sd = 3)
treatment <- rnorm(n = 120, 50, sd = 5)
db <- tibble(control, treatment) %>%
    gather(key = "random_var", value = "value")

# Graph
db %>%
ggplot(aes(value, random_var, fill=random_var))+
    geom_density_ridges(alpha=.8)
```

From the graph we can conclude that it is unlikely that the advertising is not affecting chocolate consumption since there is a noticeable difference in the means between the two groups. However, *ex-ante* in a experiment we would want to know the minimum sample size required to detect the effect of advertising, assuming that we have a good guess of the true parameters.

(it depends on the mean $\mu$, standard deviation $\sigma$, observations $n$ and a given significance level $\alpha$.)

by calculating the values that make the difference significative. Recall that power is the probability of rejecting the null hypothesis when it is actually false. Moreover, it depends on the mean $\mu$, standard deviation $\sigma$, observations $n$ and a given significance level $\alpha$.

How many chocolates are neccesary to be consumed in order to reject the null hypothesis that $H_0=40$?

We know that at a significance level $\alpha=0.05$ the Z-score is $1.96$, therefore we are looking for values that fall in the rejection region, that is:
$$
\begin{align}
Z\leq & -1.96 \\
Z\geq & 1.96
\end{align}
$$
From the Z-score function we are able to calculate the minimum values of $\bar{X}$ in order to find a statistical significant difference:

$$
\begin{align}
\frac{\bar{X}-\mu}{\sigma} & \leq -1.96 & \frac{\bar{X}-\mu}{\sigma} & \geq 1.96 \\
\newline
\frac{\bar{X}-40}{3} & \leq -1.96 & \frac{\bar{X}-40}{3} & \geq 1.96 \\
\end{align}
$$


```{r, echo=FALSE, include=FALSE}
x <- db$value[db$random_var=="control"]
```

As a result the treatment average has to be either $\bar{X} \leq$ `r round(-1.96*sd(x)+mean(x),3)` or $\bar{X} \geq$ `r round(1.96*sd(x)+mean(x),3)` in order to be statistically different from 40.

```{r}
minF <- function(x){
    low <- -1.96*sd(x)/sqrt(length(x))+mean(x)
    up <- 1.96*sd(x)/sqrt(length(x))+mean(x)
    return(c(low, up))
}

min2F <- function(mean, sd, n){
    low <- -1.96*sd/sqrt(n)+mean
    up <- 1.96*sd/sqrt(n)+mean
    return(c(low, up))
}
```



```{r}
db %>%
    filter(random_var=="control") %>%
    ggplot(aes(value))+
    geom_density(fill="black", alpha=.6)+
    scale_x_continuous(limits = c(29,50))+
    geom_vline(xintercept = c(33.4953796, 45.9189023), colour="red")
```



let's start by calculating the Z-Scores for `r1`

```{r}
zF=function(x){
    z=(x-mean(x))/(sd(x)^2/sqrt(length(x)))
    return(z)
}
```



```{r}
test <- rnorm(n = 120, 39, sd = 3) %>%
tibble(v1=., v2=db$value[db$random_var=="control"]) %>%
    gather()
t.test(test$value~test$key)
```


For now,
Suppose $\delta$ is the difference between the outcome of both subsamples, in other words $\delta=\mu_1-\mu_2$. Typically, the null hypothesis $H_0$ is that there are no difference between groups ($\delta=0$), while the alternative hypothesis $H_1$.




In order to estimate the difference, we are going to use the independent t-test given by a formula:

$$
t=\frac{\mu_1-\mu_2}{\sqrt{\frac{S_1^2}{N_1}+\frac{S_2^2}{N_2}}}
$$
```{r}
db %>%
    group_by(random_var) %>%
    summarise(mean(value), sd(value), n())
```

$$
t=\frac{39.99648-49.99105}{\sqrt{25/100000+25/100000}}=-447
$$
Which can be calculated directly by:

```{r}

t.test(db$value~db$random_var, alternative="two.sided", var.equal=F)
```





### References
