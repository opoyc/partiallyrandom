---
title: Power analysis in experimental design
author: Obryan Poyser
date: '2018-07-15'
slug: power-analysis-in-experimental-design
categories:
  - Statistics
tags:
  - Power Analysis
  - R
  - Experiments
  - Causal inference
type: post
description: ""
keywords:
  - key
  - words
topics:
  - topic 1
bibliography: allrefs.bib
---



<p>A couple of weeks ago we had hour yearly Ph.D. workshop in which we present our project development. I’ve always moral sentiments around my duties regarding what someone like me who received a grant should do. One of these moral burdens is to assist to every single presentation and try to provide useful feedback.</p>
<p>Long story short, a fellow Ph.D. student presented a draft of a experimental design. Aligned with my principles, I suggested her to be more humble regarding what she wanted to test, since there were far too many hypotheses she wanted to test for (i.e. treatments) which certainly would limit her sample within and between groups. As a consequence, she replied: <em>How big do you think my sample should be?</em>, to be honest, I couldn’t provide her with an exact answer, this post is a result of my deficient feedback.</p>
<p>Experiments have been increasing in popularity in economics, as an effect of the “empirical revolution” trending as a whole as well as an increasing criticism on identification assumptions when the objective is to find a causal effect in observational data. Indeed, experiments let us identify a cause for certain phenomena easier, nevertheless, the design as well as the cost of realizing an experiment is not negligible. If something ends bad, most probably we won’t have the same resources and opportunity to rerun it, hence, we must be careful to design an experiment that maximizes our testing expectations.</p>
<p>Finding the optimal sample size is definitely one of the main concerns a researcher should have when she designs an experiment. For this matter, there exist the so-called <strong>Power Analysis</strong>, a process of estimating the minimum (optimal) sample size needed to detect an effect given a treatment at the desired significance level.</p>
<p><span class="citation">List, Sadoff, and Wagner (2011)</span> argue that there are three main aspects we should consider in order to calculate the optimal sample size:</p>
<ol style="list-style-type: decimal">
<li>Significance level</li>
<li>Power</li>
<li>Minimum detectable effect size</li>
</ol>
<p>So, let’s start by talking about significance level, by bringing back what we learned in statistics 101 about hypothesis testing:</p>
<ul>
<li><p>Type I: Probability of rejecting the null hypothesis given that it is true (false positive). Typically, it is referred as significance level <span class="math inline">\(\alpha\)</span> of the hypothesis test. <span class="math display">\[P(R|H_0=true)\]</span></p></li>
<li><p>Type II: Probability of not rejecting (accepting for notational purposes) the null hypothesis given that it is false (false negative). Commonly, this decision error is described as <span class="math inline">\(\beta\)</span>. <span class="math display">\[P(A|H_0=false)\]</span></p></li>
</ul>
<p>The power is defined as the likelihood of correctly rejecting <span class="math inline">\(H_0\)</span> when it was actually false, or <span class="math inline">\(1-\beta=1-P(A|H_0=false)\)</span>. The rule of thumb dictates a power equivalent of <span class="math inline">\(0.80\)</span>. Having all this information, it will be useful demostrate the use of power with an example for a couple of syntethic random variables.</p>
<pre class="r"><code>require(tidyverse)
require(ggridges)

# Two normal random variables with mean 40 and 50 respectively. 
r1 &lt;- rnorm(n = 100000, 40, sd = 5)
r2 &lt;- rnorm(n = 100000, 50, sd = 5)
db &lt;- tibble(r1, r2) %&gt;%
    gather(key = &quot;random_var&quot;, value = &quot;value&quot;)

# Graph
db %&gt;%
ggplot(aes(value, random_var, fill=random_var))+
    geom_density_ridges(alpha=.8)</code></pre>
<p><img src="/post/2018-07-15-power-analysis-in-experimental-design_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Assume that we have <span class="math inline">\(\mu_1\sim N(40,5)\)</span> and <span class="math inline">\(\mu_2 \sim N(50,5)\)</span> for a control and treatment groups respectively. Suppose <span class="math inline">\(\delta\)</span> is the difference between the outcome of both subsamples, in other words <span class="math inline">\(\delta=\mu_1-\mu_2\)</span>. Typically, the null hypothesis <span class="math inline">\(H_0\)</span> is that there are no difference between groups (<span class="math inline">\(\delta=0\)</span>), while the alternative hypothesis <span class="math inline">\(H_1\)</span>.</p>
<p>In order to estimate the difference, we are going to use the independent t-test given by a formula:</p>
<p><span class="math display">\[
t=\frac{\mu_1-\mu_2}{\sqrt{\frac{S_1^2}{N_1}+\frac{S_2^2}{N_2}}}
\]</span></p>
<pre class="r"><code>db %&gt;%
    group_by(random_var) %&gt;%
    summarise(mean(value), sd(value), n())</code></pre>
<pre><code>## # A tibble: 2 x 4
##   random_var `mean(value)` `sd(value)`  `n()`
##   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;
## 1 r1                  40.0        5.00 100000
## 2 r2                  50.0        4.97 100000</code></pre>
<p><span class="math display">\[
t=\frac{39.99648-49.99105}{\sqrt{25/100000+25/100000}}=-447
\]</span> Which can be calculated directly by:</p>
<pre class="r"><code>t.test(db$value~db$random_var, alternative=&quot;two.sided&quot;, var.equal=F)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  db$value by db$random_var
## t = -447.39, df = 199990, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.023005  -9.935569
## sample estimates:
## mean in group r1 mean in group r2 
##         40.00074         49.98003</code></pre>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-List2011">
<p>List, John A, Sally Sadoff, and Mathis Wagner. 2011. “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design.” <em>Experimental Economics</em> 14 (4): 439–57. doi:<a href="https://doi.org/10.1007/s10683-011-9275-7">10.1007/s10683-011-9275-7</a>.</p>
</div>
</div>
</div>
