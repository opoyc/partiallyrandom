<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Snippet on Partially Random</title>
    <link>/categories/snippet/</link>
    <description>Recent content in Snippet on Partially Random</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>opoyserc@gmail.com (Obryan Poyser)</managingEditor>
    <webMaster>opoyserc@gmail.com (Obryan Poyser)</webMaster>
    <copyright>(c) 2018 Copyright Partially Random</copyright>
    <lastBuildDate>Thu, 07 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/snippet/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>World Cup players&#39; hype on Instagram</title>
      <link>/2018/06/07/2018-06-07-collect-followers-on-instagram/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      <author>opoyserc@gmail.com (Obryan Poyser)</author>
      <guid>/2018/06/07/2018-06-07-collect-followers-on-instagram/</guid>
      <description>Javier is visiting me for a couple of days here in Barcelona. This silly guy is trying to measure the relation between social media representativeness and expectations of qualifying in the group phase of the World Cup. It seems that he is collecting the information going from page to pageâ€¦ I as a benevolent friend create a classic scraping function to make his life easier (maybe it can make your life better as well)</description>
    </item>
    
    <item>
      <title>Getting research titles from science direct</title>
      <link>/2018/05/28/2018-05-28-getting-science-direct-articles/</link>
      <pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate>
      <author>opoyserc@gmail.com (Obryan Poyser)</author>
      <guid>/2018/05/28/2018-05-28-getting-science-direct-articles/</guid>
      <description>Lately, I have been reading a lot of articles within sciencedirect website. Now, after spending several hours reading and moving through pages I realize that it is time to create a snippet to get the articles right away. Hope it helps you too!
Packages needed require(rvest) require(tidyverse) require(stringr)  Function ext_keyF &amp;lt;- function(key_word, journal){ s1 &amp;lt;- &amp;quot;https://www.sciencedirect.com/search?qs=&amp;quot; s2 &amp;lt;- &amp;quot;&amp;amp;pub=&amp;quot; s3 &amp;lt;- &amp;quot;&amp;amp;show=100&amp;amp;sortBy=relevance&amp;amp;articleTypes=FLA&amp;amp;lastSelectedFacet=articleTypes&amp;quot; key_t &amp;lt;- key_word %&amp;gt;% stringr::str_replace_all(&amp;quot; &amp;quot;, &amp;quot;%20&amp;quot;) jour_t &amp;lt;- journal %&amp;gt;% stringr::str_replace_all(&amp;quot; &amp;quot;, &amp;quot;%20&amp;quot;) %&amp;gt;% stringr::str_replace_all(&amp;quot;&amp;amp;&amp;quot;, &amp;quot;%26&amp;quot;) link &amp;lt;- paste0(s1, key_t, s2, jour_t, s3) pages &amp;lt;- read_html(link) %&amp;gt;% html_nodes(&amp;quot;.</description>
    </item>
    
    <item>
      <title>Save with time stamp</title>
      <link>/2017/04/26/2017-04-26-save-with-time-stamp/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <author>opoyserc@gmail.com (Obryan Poyser)</author>
      <guid>/2017/04/26/2017-04-26-save-with-time-stamp/</guid>
      <description>Oftenly I have to deal with modifications of a scraping code, whose outcomes change or are subject to errors. Hence, I found useful to have a way to create versions of a data frame, this is a made savts.
This simple yet useful function creates csv file with time stamp. It saves a data frame in the working directory as a csv file, separated by semicolon with a date and system time (format %d%m%y_%H%M).</description>
    </item>
    
  </channel>
</rss>