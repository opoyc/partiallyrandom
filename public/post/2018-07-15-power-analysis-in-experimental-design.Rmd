---
title: Power analysis in experimental design
author: Obryan Poyser
date: '2018-07-15'
slug: power-analysis-in-experimental-design
categories:
  - Statistics
tags:
  - Power Analysis
  - R
  - Experiments
  - Causal inference
type: post
description: ""
keywords:
  - key
  - words
topics:
  - topic 1
bibliography: allrefs.bib
---

A couple of weeks ago we had hour yearly Ph.D. workshop in which we present our project development. I've always moral sentiments around my duties regarding what someone like me who received a grant should do. One of these moral burdens is to assist to every single presentation and try to provide useful feedback.

Long story short, a fellow Ph.D. student presented a draft of a experimental design. Aligned with my principles, I suggested her to be more humble regarding what she wanted to test, since there were far too many hypotheses she wanted to test for (i.e. treatments) which certainly would limit her sample within and between groups. As a consequence, she replied: *How big do you think my sample should be?*, to be honest, I couldn't provide her with an exact answer, this post is a result of my deficient feedback.

Experiments have been increasing in popularity in economics, as an effect of the "empirical revolution" trending as a whole as well as an increasing criticism on identification assumptions when the objective is to find a causal effect in observational data. Indeed, experiments let us identify a cause for certain phenomena easier, nevertheless, the design as well as the cost of realizing an experiment is not negligible. If something ends bad, most probably we won't have the same resources and opportunity to rerun it, hence, we must be careful to design an experiment that maximizes our testing expectations. 

Finding the optimal sample size is definitely one of the main concerns a researcher should have when she designs an experiment. For this matter, there exist the so-called **Power Analysis**, a process of estimating the minimum (optimal) sample size needed to detect an effect given a treatment at the desired significance level.  


@List2011[] argue that there are three main aspects we should consider in order to calculate the optimal sample size:

1. Significance level
1. Power
1. Minimum detectable effect size

So, let's start by talking about significance level, by bringing back what we learned in statistics 101 about hypothesis testing:

+ Type I: Probability of rejecting the null hypothesis given that it is true (false positive). Typically, it is referred as significance level $\alpha$ of the hypothesis test.
$$P(R|H_0=true)$$

+ Type II: Probability of not rejecting (accepting for notational purposes) the null hypothesis given that it is false (false negative). Commonly, this decision error is described as $\beta$. 
$$P(A|H_0=false)$$

The power is defined as the likelihood of correctly rejecting $H_0$ when it was actually false, or $1-\beta=1-P(A|H_0=false)$. The rule of thumb dictates a power equivalent of $0.80$. Having all this information, it will be useful demostrate the use of power with an example for a couple of syntethic random variables.

```{r, warning=FALSE, message=FALSE}
require(tidyverse)
require(ggridges)

# Two normal random variables with mean 40 and 50 respectively. 
r1 <- rnorm(n = 100000, 40, sd = 5)
r2 <- rnorm(n = 100000, 50, sd = 5)
db <- tibble(r1, r2) %>%
    gather(key = "random_var", value = "value")

# Graph
db %>%
ggplot(aes(value, random_var, fill=random_var))+
    geom_density_ridges(alpha=.8)
```

Assume that we have $\mu_1\sim N(40,5)$ and $\mu_2 \sim N(50,5)$ for a control and treatment groups respectively. Suppose $\delta$ is the difference between the outcome of both subsamples, in other words $\delta=\mu_1-\mu_2$. Typically, the null hypothesis $H_0$ is that there are no difference between groups ($\delta=0$), while the alternative hypothesis $H_1$. 





In order to estimate the difference, we are going to use the independent t-test given by a formula:

$$
t=\frac{\mu_1-\mu_2}{\sqrt{\frac{S_1^2}{N_1}+\frac{S_2^2}{N_2}}}
$$
```{r}
db %>%
    group_by(random_var) %>%
    summarise(mean(value), sd(value), n())
```

$$
t=\frac{39.99648-49.99105}{\sqrt{25/100000+25/100000}}=-447
$$
Which can be calculated directly by:

```{r}

t.test(db$value~db$random_var, alternative="two.sided", var.equal=F)
```





### References